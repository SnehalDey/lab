{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing the needed datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                          4013\n",
      "Housing Situation                0\n",
      "Crime                            0\n",
      "Experience                       0\n",
      "Satisfation with employer    38087\n",
      "Gender                       74127\n",
      "Age                              0\n",
      "Country                          0\n",
      "Size of City                     0\n",
      "Profession                    2853\n",
      "University Degree            80600\n",
      "Wears Glasses                    0\n",
      "Hair Color                   70211\n",
      "Height                           0\n",
      "Extra                            0\n",
      "Income                           0\n",
      "dtype: int64\n",
      "(1048187, 16)\n",
      "Year                           5492\n",
      "Housing Situation                 0\n",
      "Crime                             0\n",
      "Experience                        0\n",
      "Satisfation with employer     51485\n",
      "Gender                       100244\n",
      "Age                               0\n",
      "Country                           1\n",
      "Size of City                      0\n",
      "Profession                     3507\n",
      "University Degree            109309\n",
      "Wears Glasses                     0\n",
      "Hair Color                    94824\n",
      "Height                            0\n",
      "Extra                             0\n",
      "dtype: int64\n",
      "(1048187, 15)\n",
      "(369438, 15)\n",
      "(1417625, 15)\n",
      "(1048187, 1)\n",
      "(1417625, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#importing dataset\n",
    "dataset=pd.read_csv('C:/Users/BadBoy/Desktop/kaggle2/data.csv',low_memory=False)\n",
    "dataset=dataset.drop(\"Instance\",axis=1)\n",
    "testset=pd.read_csv('C:/Users/BadBoy/Desktop/kaggle2/test.csv',low_memory=False)\n",
    "testset=testset.drop(\"Instance\",axis=1)\n",
    "#merge=pd.concat([dataset,testset])\n",
    "dataset.Gender.unique()\n",
    "\n",
    "#renaming the columns for convenience\n",
    "\n",
    "dataset = dataset.rename(columns={'Year of Record': 'Year'})\n",
    "dataset = dataset.rename(columns={'Yearly Income in addition to Salary (e.g. Rental Income)': 'Extra'})\n",
    "dataset = dataset.rename(columns={'Work Experience in Current Job [years]': 'Experience'})\n",
    "dataset = dataset.rename(columns={'Crime Level in the City of Employement':'Crime'})\n",
    "dataset= dataset.rename(columns={'Total Yearly Income [EUR]':'Income'})\n",
    "dataset= dataset.rename(columns={'Body Height [cm]':'Height'})\n",
    "\n",
    "testset = testset.rename(columns={'Year of Record': 'Year'})\n",
    "testset = testset.rename(columns={'Yearly Income in addition to Salary (e.g. Rental Income)': 'Extra'})\n",
    "testset = testset.rename(columns={'Work Experience in Current Job [years]': 'Experience'})\n",
    "testset = testset.rename(columns={'Crime Level in the City of Employement':'Crime'})\n",
    "testset = testset.rename(columns={'Total Yearly Income [EUR]':'Income'})\n",
    "testset = testset.rename(columns={'Body Height [cm]':'Height'})\n",
    "\n",
    "#scaling the Income column  with log\n",
    "dataset['Income'] = dataset['Income'].apply(np.log)\n",
    "\n",
    "#converting 'f' and 'female' into 'female'\n",
    "\n",
    "dataset['Gender'].replace(['f','female'],'female',inplace=True)\n",
    "testset['Gender'].replace(['f','female'],'female',inplace=True)\n",
    "\n",
    "#changing Extra column to int by removing EUR\n",
    "dataset['Extra']=dataset['Extra'].str.replace('\\D+','').astype(int)\n",
    "testset['Extra']=testset['Extra'].str.replace('\\D+','').astype(int)\n",
    "\n",
    "#finidng if there is any null value in the dataset\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "#dropping if both dataset and preofession is a null value\n",
    "index_to_drop = dataset[((dataset.Gender == 'unknown') | (pd.isnull(dataset.Gender))) & ((pd.isnull(dataset.Profession)))].index\n",
    "dataset.drop(index_to_drop,inplace= True)\n",
    "print(dataset.shape)\n",
    "\n",
    "y = dataset['Income']\n",
    "y=pd.DataFrame(y)\n",
    "\n",
    "# concatinating the dataset and test set into one dataset\n",
    "combine=pd.concat([dataset, testset])\n",
    "y_only = combine['Income']\n",
    "y_only=pd.DataFrame(y_only)\n",
    "\n",
    "# dropping the income columns from the dataset and combining it\n",
    "train_model = dataset.drop(columns =['Income'])\n",
    "test_model =  testset.drop(columns =['Income'])                                                     \n",
    "combined_df = pd.concat([train_model, test_model])\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "print(train_model.shape)\n",
    "print(test_model.shape)\n",
    "print(combined_df.shape)\n",
    "print(y.shape)\n",
    "print(y_only.shape)\n",
    "\n",
    "\n",
    "#preprocessing data and converting all the nan values to 'Missing'\n",
    "imp_gender = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='Missing')\n",
    "imp_Satisfation = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='Missing')\n",
    "imp_university = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='Missing')\n",
    "imp_profession = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value='Missing')\n",
    "\n",
    "\n",
    "imp_Satisfation = imp_Satisfation.fit(combined_df[['Satisfation with employer']])\n",
    "combined_df['Satisfation with employer'] = imp_Satisfation.transform(combined_df[['Satisfation with employer']]).ravel()\n",
    "\n",
    "imp_gender = imp_gender.fit(combined_df[['Gender']])\n",
    "combined_df['Gender'] = imp_gender.transform(combined_df[['Gender']]).ravel()\n",
    "\n",
    "imp_university = imp_university.fit(combined_df[['University Degree']])\n",
    "combined_df['University Degree'] = imp_university.transform(combined_df[['University Degree']]).ravel()\n",
    "\n",
    "imp_profession = imp_university.fit(combined_df[['Profession']])\n",
    "combined_df['Profession'] = imp_university.transform(combined_df[['Profession']]).ravel()\n",
    "\n",
    "\n",
    "#replacing the \"0\",'other','unknown' with 'missing' for the categorical columns\n",
    "combined_df['Gender'].replace(['0','other','unknown'],'Missing',inplace = True)\n",
    "combined_df['University Degree'].replace('0','No'],'Missing',inplace = True)\n",
    "combined_df['Housing Situation'].replace(['0','nA'],'Missing',inplace = True)\n",
    "#converting th e column into float and calulating the mean\n",
    "avg_year = combined_df[\"Year\"].astype(\"float\").mean(axis=0)\n",
    "#converting th e column into float and calculating the mean\n",
    "avg_age = combined_df[\"Age\"].astype(\"float\").mean(axis=0)\n",
    "\n",
    "#replacing nan values by average.\n",
    "combined_df[\"Year\"].replace(np.nan, avg_year, inplace=True)\n",
    "combined_df[\"Age\"].replace(np.nan, avg_age, inplace=True)\n",
    "\n",
    "\n",
    "combined_df.Experience.replace([\"#NUM!\"], np.nan, inplace=True)\n",
    "combined_df['Experience']=combined_df['Experience'].fillna(combined_df.Experience.median())\n",
    "avg_exp = combined_df[\"Experience\"].astype(\"float\").mean(axis=0)\n",
    "#taking into concideration only those countries which have been metioned more than 100 times and the age is less than 100.\n",
    "#The countries which are present less than 100 times marking the, as notimportant.\n",
    "countries_greaterthan100 = combined_df.Country.value_counts()[combined_df.Country.value_counts() > 100]\n",
    "countrylist = countries_greaterthan100.index.tolist()\n",
    "combined_df.Country.where(combined_df.Country.isin(countrylist),'notimportant',inplace = True)\n",
    "combined_df.Age.where(combined_df.Age <= 100,inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping few columns\n",
    "combined_df=combined_df.drop(['Wears Glasses','Hair Color'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeblock frequency encoding: Year_Size of City\n",
      "timeblock frequency encoding: Year_Height\n",
      "timeblock frequency encoding: Year_Experience\n",
      "timeblock frequency encoding: Year_Crime\n",
      "timeblock frequency encoding: Year_Extra\n",
      "timeblock frequency encoding: Country_Size of City\n",
      "timeblock frequency encoding: Country_Height\n",
      "timeblock frequency encoding: Country_Experience\n",
      "timeblock frequency encoding: Country_Crime\n",
      "timeblock frequency encoding: Country_Extra\n",
      "timeblock frequency encoding: Profession_Size of City\n",
      "timeblock frequency encoding: Profession_Height\n",
      "timeblock frequency encoding: Profession_Experience\n",
      "timeblock frequency encoding: Profession_Crime\n",
      "timeblock frequency encoding: Profession_Extra\n",
      "timeblock frequency encoding: University Degree_Size of City\n",
      "timeblock frequency encoding: University Degree_Height\n",
      "timeblock frequency encoding: University Degree_Experience\n",
      "timeblock frequency encoding: University Degree_Crime\n",
      "timeblock frequency encoding: University Degree_Extra\n",
      "timeblock frequency encoding: Satisfation with employer_Size of City\n",
      "timeblock frequency encoding: Satisfation with employer_Height\n",
      "timeblock frequency encoding: Satisfation with employer_Experience\n",
      "timeblock frequency encoding: Satisfation with employer_Crime\n",
      "timeblock frequency encoding: Satisfation with employer_Extra\n",
      "timeblock frequency encoding: Housing Situation_Size of City\n",
      "timeblock frequency encoding: Housing Situation_Height\n",
      "timeblock frequency encoding: Housing Situation_Experience\n",
      "timeblock frequency encoding: Housing Situation_Crime\n",
      "timeblock frequency encoding: Housing Situation_Extra\n"
     ]
    }
   ],
   "source": [
    "#concating the columns to increase the number of features\n",
    "def create_cat_con(df,cats,cons,normalize=True):\n",
    "    for i,cat in enumerate(cats):\n",
    "        vc = df[cat].value_counts(dropna=False, normalize=normalize).to_dict()\n",
    "        nm = cat + '_FE_FULL'\n",
    "        df[nm] = df[cat].map(vc)\n",
    "        df[nm] = df[nm].astype('float32')\n",
    "        for j,con in enumerate(cons):\n",
    "#             print(\"cat %s con %s\"%(cat,con))\n",
    "            new_col = cat +'_'+ con\n",
    "            print('timeblock frequency encoding:', new_col)\n",
    "            df[new_col] = df[cat].astype(str)+'_'+df[con].astype(str)\n",
    "            temp_df = df[new_col]\n",
    "            fq_encode = temp_df.value_counts(normalize=True).to_dict()\n",
    "            df[new_col] = df[new_col].map(fq_encode)\n",
    "            df[new_col] = df[new_col]/df[cat+'_FE_FULL']\n",
    "    return df\n",
    "\n",
    "cats = ['Year', 'Country',\n",
    "        'Profession', 'University Degree',\n",
    "        'Satisfation with employer','Housing Situation','Gender' ]\n",
    "\n",
    "cons = ['Size of City','Height','Experience', 'Crime','Extra',]\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "\n",
    "#performing targert encoding on the categorical columns\n",
    "combined_df = create_cat_con(combined_df,cats,cons)\n",
    "\n",
    "feat_le =ce.TargetEncoder(cols=['Country','Profession','University Degree','Satisfation with employer','Gender'])\n",
    "feat_le.fit(combined_df,y_only)\n",
    "combined_df = feat_le.transform(combined_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data using min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "combined_df = pd.DataFrame(scaler.fit_transform(combined_df), columns=combined_df.columns, index=combined_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country_FE_FULL',\n",
       " 'Satisfation with employer_Extra',\n",
       " 'Country_Extra',\n",
       " 'Housing Situation_Experience',\n",
       " 'Age_Crime',\n",
       " 'Hair Color',\n",
       " 'University Degree_Extra',\n",
       " 'Profession_FE_FULL',\n",
       " 'Profession_Crime',\n",
       " 'Satisfation with employer',\n",
       " 'Housing Situation_Crime',\n",
       " 'Year_Size of City',\n",
       " 'Age_Height',\n",
       " 'Experience',\n",
       " 'Country_Experience',\n",
       " 'Year_Experience',\n",
       " 'Satisfation with employer_Height',\n",
       " 'Age_Experience',\n",
       " 'University Degree_FE_FULL',\n",
       " 'Satisfation with employer_Size of City',\n",
       " 'University Degree_Size of City',\n",
       " 'Age_FE_FULL',\n",
       " 'Year',\n",
       " 'Housing Situation_Height',\n",
       " 'Housing Situation',\n",
       " 'Country_Crime',\n",
       " 'Country_Height',\n",
       " 'University Degree_Height',\n",
       " 'Year_FE_FULL',\n",
       " 'Crime',\n",
       " 'Housing Situation_Size of City',\n",
       " 'Country',\n",
       " 'Age',\n",
       " 'Height',\n",
       " 'Profession_Experience',\n",
       " 'University Degree_Crime',\n",
       " 'Housing Situation_FE_FULL',\n",
       " 'Profession',\n",
       " 'Year_Extra',\n",
       " 'Gender',\n",
       " 'Country_Size of City',\n",
       " 'Satisfation with employer_FE_FULL',\n",
       " 'Profession_Height',\n",
       " 'University Degree_Experience',\n",
       " 'Age_Extra',\n",
       " 'Satisfation with employer_Experience',\n",
       " 'Wears Glasses',\n",
       " 'Housing Situation_Extra',\n",
       " 'Age_Size of City',\n",
       " 'Size of City',\n",
       " 'Satisfation with employer_Crime',\n",
       " 'Profession_Size of City',\n",
       " 'Extra',\n",
       " 'University Degree',\n",
       " 'Year_Crime',\n",
       " 'Year_Height',\n",
       " 'Profession_Extra']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_col =  list(set(combined_df))\n",
    "features_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's l2: 0.60333\tvalid_1's l2: 0.606496\n",
      "[2000]\ttraining's l2: 0.202952\tvalid_1's l2: 0.204691\n",
      "[3000]\ttraining's l2: 0.117629\tvalid_1's l2: 0.118838\n",
      "[4000]\ttraining's l2: 0.0901986\tvalid_1's l2: 0.0912165\n",
      "[5000]\ttraining's l2: 0.0768179\tvalid_1's l2: 0.0777134\n",
      "[6000]\ttraining's l2: 0.069296\tvalid_1's l2: 0.0701322\n",
      "[7000]\ttraining's l2: 0.0640748\tvalid_1's l2: 0.0648782\n",
      "[8000]\ttraining's l2: 0.0604983\tvalid_1's l2: 0.0612956\n",
      "[9000]\ttraining's l2: 0.0580744\tvalid_1's l2: 0.0588705\n",
      "[10000]\ttraining's l2: 0.0562116\tvalid_1's l2: 0.0570278\n",
      "[11000]\ttraining's l2: 0.0548781\tvalid_1's l2: 0.0557061\n",
      "[12000]\ttraining's l2: 0.0537774\tvalid_1's l2: 0.0546312\n",
      "[13000]\ttraining's l2: 0.0527368\tvalid_1's l2: 0.0536367\n",
      "[14000]\ttraining's l2: 0.0516844\tvalid_1's l2: 0.0526322\n",
      "[15000]\ttraining's l2: 0.0509648\tvalid_1's l2: 0.0519869\n",
      "[16000]\ttraining's l2: 0.0503313\tvalid_1's l2: 0.0514177\n",
      "[17000]\ttraining's l2: 0.0497911\tvalid_1's l2: 0.0509453\n",
      "[18000]\ttraining's l2: 0.0492982\tvalid_1's l2: 0.0505089\n",
      "[19000]\ttraining's l2: 0.0488818\tvalid_1's l2: 0.0501595\n",
      "[20000]\ttraining's l2: 0.0484401\tvalid_1's l2: 0.0497848\n",
      "[21000]\ttraining's l2: 0.0480214\tvalid_1's l2: 0.0494078\n",
      "[22000]\ttraining's l2: 0.0476162\tvalid_1's l2: 0.0490454\n",
      "[23000]\ttraining's l2: 0.0472304\tvalid_1's l2: 0.0487093\n",
      "[24000]\ttraining's l2: 0.0468656\tvalid_1's l2: 0.0484275\n",
      "[25000]\ttraining's l2: 0.0465389\tvalid_1's l2: 0.0481913\n",
      "[26000]\ttraining's l2: 0.0462432\tvalid_1's l2: 0.0479791\n",
      "[27000]\ttraining's l2: 0.0459685\tvalid_1's l2: 0.0477943\n",
      "[28000]\ttraining's l2: 0.0457071\tvalid_1's l2: 0.047619\n",
      "[29000]\ttraining's l2: 0.0454415\tvalid_1's l2: 0.0474196\n",
      "[30000]\ttraining's l2: 0.0451601\tvalid_1's l2: 0.0472126\n",
      "[31000]\ttraining's l2: 0.0449039\tvalid_1's l2: 0.0470383\n",
      "[32000]\ttraining's l2: 0.0446805\tvalid_1's l2: 0.0468827\n",
      "[33000]\ttraining's l2: 0.0444749\tvalid_1's l2: 0.0467456\n",
      "[34000]\ttraining's l2: 0.0442622\tvalid_1's l2: 0.0465966\n",
      "[35000]\ttraining's l2: 0.0440576\tvalid_1's l2: 0.0464587\n",
      "[36000]\ttraining's l2: 0.0438601\tvalid_1's l2: 0.0463202\n",
      "[37000]\ttraining's l2: 0.0436584\tvalid_1's l2: 0.0461776\n",
      "[38000]\ttraining's l2: 0.0434674\tvalid_1's l2: 0.0460408\n",
      "[39000]\ttraining's l2: 0.0432444\tvalid_1's l2: 0.0458886\n",
      "[40000]\ttraining's l2: 0.0430331\tvalid_1's l2: 0.0457441\n",
      "[41000]\ttraining's l2: 0.0428331\tvalid_1's l2: 0.0456074\n",
      "[42000]\ttraining's l2: 0.0426451\tvalid_1's l2: 0.0454735\n",
      "[43000]\ttraining's l2: 0.0424612\tvalid_1's l2: 0.0453408\n",
      "[44000]\ttraining's l2: 0.042285\tvalid_1's l2: 0.0452113\n",
      "[45000]\ttraining's l2: 0.0421142\tvalid_1's l2: 0.0450733\n",
      "[46000]\ttraining's l2: 0.0419481\tvalid_1's l2: 0.0449461\n",
      "[47000]\ttraining's l2: 0.0417683\tvalid_1's l2: 0.0448086\n",
      "[48000]\ttraining's l2: 0.0416004\tvalid_1's l2: 0.044685\n",
      "[49000]\ttraining's l2: 0.0414282\tvalid_1's l2: 0.0445604\n",
      "[50000]\ttraining's l2: 0.0412655\tvalid_1's l2: 0.0444447\n",
      "[51000]\ttraining's l2: 0.041113\tvalid_1's l2: 0.0443424\n",
      "[52000]\ttraining's l2: 0.040966\tvalid_1's l2: 0.044257\n",
      "[53000]\ttraining's l2: 0.040824\tvalid_1's l2: 0.044164\n",
      "[54000]\ttraining's l2: 0.0406962\tvalid_1's l2: 0.0440773\n",
      "[55000]\ttraining's l2: 0.0405681\tvalid_1's l2: 0.0439985\n",
      "[56000]\ttraining's l2: 0.0404436\tvalid_1's l2: 0.0439239\n",
      "[57000]\ttraining's l2: 0.0403067\tvalid_1's l2: 0.0438364\n",
      "[58000]\ttraining's l2: 0.0401718\tvalid_1's l2: 0.0437556\n",
      "[59000]\ttraining's l2: 0.0400427\tvalid_1's l2: 0.0436766\n",
      "[60000]\ttraining's l2: 0.0399075\tvalid_1's l2: 0.0436001\n",
      "[61000]\ttraining's l2: 0.0397871\tvalid_1's l2: 0.0435304\n",
      "[62000]\ttraining's l2: 0.0396722\tvalid_1's l2: 0.0434691\n",
      "[63000]\ttraining's l2: 0.039553\tvalid_1's l2: 0.0434073\n",
      "[64000]\ttraining's l2: 0.0394397\tvalid_1's l2: 0.0433503\n",
      "[65000]\ttraining's l2: 0.0393259\tvalid_1's l2: 0.0432929\n",
      "[66000]\ttraining's l2: 0.0392152\tvalid_1's l2: 0.0432393\n",
      "[67000]\ttraining's l2: 0.0391007\tvalid_1's l2: 0.0431818\n",
      "[68000]\ttraining's l2: 0.0389914\tvalid_1's l2: 0.0431328\n",
      "[69000]\ttraining's l2: 0.0388834\tvalid_1's l2: 0.0430783\n",
      "[70000]\ttraining's l2: 0.0387774\tvalid_1's l2: 0.0430229\n",
      "[71000]\ttraining's l2: 0.0386812\tvalid_1's l2: 0.042973\n",
      "[72000]\ttraining's l2: 0.0385868\tvalid_1's l2: 0.0429234\n",
      "[73000]\ttraining's l2: 0.0384918\tvalid_1's l2: 0.0428778\n",
      "[74000]\ttraining's l2: 0.0383973\tvalid_1's l2: 0.0428349\n",
      "[75000]\ttraining's l2: 0.0382987\tvalid_1's l2: 0.0427895\n",
      "[76000]\ttraining's l2: 0.0382059\tvalid_1's l2: 0.0427483\n",
      "[77000]\ttraining's l2: 0.038113\tvalid_1's l2: 0.042714\n",
      "[78000]\ttraining's l2: 0.0380195\tvalid_1's l2: 0.042676\n",
      "[79000]\ttraining's l2: 0.0379248\tvalid_1's l2: 0.0426393\n",
      "[80000]\ttraining's l2: 0.0378372\tvalid_1's l2: 0.0426051\n",
      "[81000]\ttraining's l2: 0.0377549\tvalid_1's l2: 0.0425762\n",
      "[82000]\ttraining's l2: 0.0376698\tvalid_1's l2: 0.0425476\n",
      "[83000]\ttraining's l2: 0.0375857\tvalid_1's l2: 0.0425175\n",
      "[84000]\ttraining's l2: 0.0375033\tvalid_1's l2: 0.0424866\n",
      "[85000]\ttraining's l2: 0.0373984\tvalid_1's l2: 0.0424557\n",
      "[86000]\ttraining's l2: 0.0372942\tvalid_1's l2: 0.0424165\n",
      "[87000]\ttraining's l2: 0.0372072\tvalid_1's l2: 0.0423834\n",
      "[88000]\ttraining's l2: 0.0371176\tvalid_1's l2: 0.0423478\n",
      "[89000]\ttraining's l2: 0.0370347\tvalid_1's l2: 0.0423148\n",
      "[90000]\ttraining's l2: 0.0369488\tvalid_1's l2: 0.042282\n",
      "[91000]\ttraining's l2: 0.0368626\tvalid_1's l2: 0.042254\n",
      "[92000]\ttraining's l2: 0.0367632\tvalid_1's l2: 0.0422267\n",
      "[93000]\ttraining's l2: 0.0366721\tvalid_1's l2: 0.0422012\n",
      "[94000]\ttraining's l2: 0.0365689\tvalid_1's l2: 0.0421729\n",
      "[95000]\ttraining's l2: 0.0364859\tvalid_1's l2: 0.0421452\n",
      "[96000]\ttraining's l2: 0.0364051\tvalid_1's l2: 0.042119\n",
      "[97000]\ttraining's l2: 0.0363267\tvalid_1's l2: 0.042096\n",
      "[98000]\ttraining's l2: 0.0362491\tvalid_1's l2: 0.0420721\n",
      "[99000]\ttraining's l2: 0.0361692\tvalid_1's l2: 0.0420464\n",
      "[100000]\ttraining's l2: 0.0360919\tvalid_1's l2: 0.0420225\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100000]\ttraining's l2: 0.0360919\tvalid_1's l2: 0.0420225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "518526825.4462467"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the dataset and training the model\n",
    "X_train,X_test = combined_df[features_col].iloc[:1048187],combined_df[features_col].iloc[1048187:]\n",
    "Y_train=dataset['Income'].iloc[:1048187]\n",
    "x_train,x_val,y_train,y_val = train_test_split(X_train,Y_train,test_size=0.2,random_state=1234)\n",
    "params = {\n",
    "          'max_depth': 20,\n",
    "          'learning_rate': 0.001,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mse',\n",
    "          \"verbosity\": -1,\n",
    "         }\n",
    "trn_data = lgb.Dataset(x_train, label=y_train)\n",
    "val_data = lgb.Dataset(x_val, label=y_val)\n",
    "# test_data = lgb.Dataset(X_test)\n",
    "clf = lgb.train(params, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n",
    "#predicting using our model\n",
    "pre_test_lgb = clf.predict(X_test)\n",
    "'done'\n",
    "#calulating the mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pre_val_lgb = clf.predict(x_val)\n",
    "val_mse = mean_squared_error(np.exp(y_val),np.exp(pre_val_lgb))\n",
    "val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22771.184102857864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(val_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading the result into the csv file\n",
    "submit=pd.read_csv('C:/Users/BadBoy/Desktop/kaggle2/submit.csv',low_memory=False)\n",
    "solution= pd.DataFrame ({'Instance':  submit['Instance'],'Total Yearly Income [EUR]': np.exp(pre_test_lgb)})\n",
    "solution.to_csv(r'C:/Users/BadBoy/Desktop/kaggle2/sol_best.csv'', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
